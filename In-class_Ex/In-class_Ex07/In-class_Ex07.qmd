---
title: "In Class Exercise 6: Global and Local Measures of Autocorrelation"
date-modified: "`r Sys.Date()`"
date: "20 Feburary 2023"
execute: 
  message: false
  warning: false
editor: visual
---

# Setup

```{r}
pacman::p_load(sf, tmap, sfdep, zoo, tidyverse)
```

## Import Data

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

Attribute Data

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

```{r}
hunan_GDPPC <- left_join(hunan,hunan2012) %>%
  select(1:4, 7, 9, 15)
```

## Step 1: Queen's method

Prepare weight metrics first

```{r}
wm_q <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb, 
                         style = "W"),
         .before = 1)
```

### Compute Global Moran's I

```{r}
moranI <- global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

### Perform Global Moran's I

Gives you test statistics , p value for you to find

```{r}
global_moran_test(wm_q$GDPPC,
                  wm_q$nb,
                  wm_q$wt)
```

0.000001095 is smaller than the alpha value of 0.05 hence we reject the H0 that the observed gdppc is spatially independent.

Moran I Statistic value is greater than 0, hence there are signed of clustering. \### Perform Global Moran's I Permutation Test Ensure that your model is reproducible:

```{r}
set.seed(1234)
```

```{r}
global_moran_perm(wm_q$GDPPC, 
                  wm_q$nb,
                  wm_q$wt, 
                  nsim = 99)
```

0.30075 is the moran's statistic Significance level (p value) has changed

## Computing Local Moran's I

```{r}
lisa <- wm_q %>%
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim = 99),
    .before = 1) %>% # push everything in front
  unnest(local_moran) # so that we can map later
lisa
```

-   ii: local moran i statistics

-   eii: expectations, std dev

-   var: var of moran i

-   z: standardised moran i

-   p_ii: p-value of i

-   p_ii_sim: p-value after simulation

-   p folded: k fold method for simulation

-   mean and pysal (python library) should be the same, you can use either mean or pysal.

### Visualising Local Moran's I

```{r}
tmap_mode('plot')
tm_shape(lisa) + 
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits = c(6,8))
```

```{r}
tmap_mode('plot')
tm_shape(lisa) + 
  tm_fill("p_ii_sim") +  # p-value of local moran i
  tm_borders(alpha = 0.5)
```

::: callout-note
Should ideally use the one from the simulation (e.g. p_ii_sim r p_folded_sim) to get a more stable result.
:::

Combining the two

```{r}
lisa_sig <- lisa %>%
  filter(p_ii < 0.05) # take out those that are < 0.05

tmap_mode('plot')
tm_shape(lisa)+ 
  tm_polygons() + 
  tm_borders(alpha = 0.5) +
  tm_shape(lisa_sig)+
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

low-high and high-low are outliers.

Not tidy \^ figure out from hands on ex. Should also have a portion called insignificant.

## Deriving contiguity weight: Rook's method

```{r}
HCSA <- wm_q %>% 
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_Gi)
HCSA
```

Local G: where ii = 0, exclude yourself

Local G\*: Include yourself, hn99

## Visualising Gi\*

```{r}
# tmap_mode("view") 
# tm_shape(HCSA) + 
#  tm_fill("gi_star") +
#  tm_borders(alpha = 0.5) + 
#  tm_view(c(6,8))
```

Visualing p value of HCSA

```{r}
tmap_mode("plot")
tm_shape(HCSA) + 
  tm_fill("p_sim") +
  tm_borders(alpha = 0.5)
```

# Emerging Hot Spot Analysis

Year, location name, value Consolidate

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

```{r}
GDPPC_st <- spacetime(GDPPC, 
                      hunan, 
                      .loc_col = "County", 
                      .time_col = "Year")
```

```{r}
GDPPC_nb <-GDPPC_st %>%
  activate("geometry") %>%
  mutate(
    nb = include_self(st_contiguity(geometry)),
    wt= st_weights(nb)
  ) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

## Computing Gi\*

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt, nsim = 99)) %>%
  tidyr::unnest(gi_star)
```

# Mann-Kendall Test

```{r}
cbg <- gi_stars %>% 
  ungroup() %>%
  filter(County == "Changsha") |>
  select(County, Year, gi_star)
```

```{r}
#emerging <- ehsa %>%
#  arrange(s1,abs(tau)) %>%
#  slice(1:5)
```

## Performing Emerging Hotspot Analysis

```{r}
#ehsa <- emerging_hotspot_analysis(
#  x = GDPP_st,
#  .var = "GDPPC",
#  k = 1,
#  nsim = 99
#)
```

```{r}
# ggplot(data = ehsa,
#       aes(x = classification)) + 
#  geom_bar()
```

## Visualing EHSA

```{r}
#| eval: false
# lisa_sig <- lisa %>% 
#  filter(p_li < 0.05)
#tmap_mode('plot')
#tm_shape(lisa) + 
#  tm_polygons() + 
#  tm_borders(alpha = 0.5) + 
#  tm_shape(lisa_sig) + 
#  tm_fill("mean") + 
#  tm_borders(alpha = 0.4)
```

```{r}
#p <- gplot(data = cbg,
# aes (x = Year, y = gi_star)) + 
#  geom_line() + 
#  theme_light()
```

ggplotly(p) - create interactive

```{r}
#cbg %<% 
#  sumarise
```
