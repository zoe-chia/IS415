---
title: "Hands-on_Ex07: Geographically Weighted Regression"
date-modified: "`r Sys.Date()`"
date: "6 March 2023"
execute: 
  message: false
  warning: false
editor: visual
---

# Setup

```{r}
pacman::p_load(dplyr, olsrr, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, readr, corrplot)
```

## Importing data

### Geospatial Data

The shapefile consists of URA Master Plan 2014's planning subzone boundaries. The GIS data is in svy21 projected coordinates systems, and does not have EPSG information. We hence have to update it with the correct EPSG cod3, 3414.

```{r}
mpsz = st_read(
  dsn = "data/geospatial", 
  layer = "MP14_SUBZONE_WEB_PL") %>%
  st_transform(crs = 3414)
```

**Verify the projection**

```{r}
st_crs(mpsz)
```

```{r}
st_bbox(mpsz) #view extent
```

### Aspatial Data

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
```

```{r}
glimpse(condo_resale)
```

**Convert aspatial df to sf**

-   st_as_sf() to convert tibble dataframe to simple feature data frame.

-   st_transform() to convert coordinates from wgs84 (crs=4326) to svy21 (crs=3414).

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

# Exploratory Data Analysis (EDA)

## Distribution of SELLING PRICE

```{r}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

-   The distribution of `SELLING_PRICE` is right skewed, which means that more condominiums were transacted at a lower price.

### Normalise the distribution with LOG

1.  Normalise

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

2.  Plot

```{r}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

The distribution becomes less skewed after the transformation.

## Plot Multiple Histograms with Different Variables

1.  Create 12 histograms with ggplot() + geom_histogram()
2.  `ggarrange()` is used to organise the histograms into 3 columns and 4 rows.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

# ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
#          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
#          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
#         ncol = 3, nrow = 4)
```

## Drawing Statistical Point Map

1.  Set to interactive mode

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
```

2.  Create interactive point symbol map

```{r}
tm_shape(mpsz)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

# Hedonic Pricing Modeling

## (A) Simple Linear Regression Method

We will build a simple linear regression model by using `SELLING_PRICE` as the dependent variable and `AREA_SQM` as the independent variable.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

`lm()` returns an object of class "lm" or for multiple responses of class, c("mlm", "lm").

The functions `summary()` and `anova()` print a summary and analysis of variance table of results.

```{r}
summary(condo.slr)
```

::: callout-note
The output report reveals that the SELLING_PRICE can be explained by the following formula:

y = -258121.1 + 14719 x

The **R-squared** of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.

Since **p-value** is much **smaller** than 0.0001, we will **reject** the null hypothesis that mean is a good estimator of `SELLING_PRICE`.

The above model is hence a **good estimator** of `SELLING_PRICE`.
:::

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

The figure above shows that there are a few statistical outliers with relatively high selling prices.

# (B) Multiple Linear Regression

## Visualising the relationships of the independent variables

-   Avoid multicollinearity: Ensure that variables that are highly correlated to each other are not used.

-   Methods in `corrplot`: "AOE", "FPC", "hclust", "alphabet"

    -   We use AOE, which orders the variables by using the angular order of the eigenvectors method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

```{r}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

::: callout-note
From the scatterplot matrix, it is clear that ***Freehold*** is highly correlated to ***LEASEHOLD_99YR***. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, ***LEASEHOLD_99YR*** is excluded in the subsequent model building.
:::

## Building a Hedonic Pricing Model using MLR

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

From the above report it is clear that not all the independent variables are statistically significant. We will revise the model by removing those variables which are not statistically significant.

## Preparing Publication Quality Table: olsrr

Calibrate the revised model by using the code chunk below:

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

## Preparing Publication Quality Table: gtsummary

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) package provides an elegant and flexible way to create publication-ready summary tables in R.

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
# tbl_regression(condo.mlr1, intercept = TRUE)
```

```{r}
# tbl_regression(condo.mlr1, 
#               intercept = TRUE) %>% 
#  add_glance_source_note(
#    label = list(sigma ~ "\U03C3"),
#    include = c(r.squared, adj.r.squared, 
#                AIC, statistic,
#                p.value, sigma))
```

## Check for multicollinearity

In this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called [**olsrr**](https://olsrr.rsquaredacademy.com/). It provides a collection of very useful methods for building better multiple linear regression models:

-   comprehensive regression output

-   residual diagnostics

-   measures of influence

-   heteroskedasticity tests

-   collinearity diagnostics

-   model fit assessment

-   variable contribution assessment

-   variable selection procedures

```{r}
ols_vif_tol(condo.mlr1)
```

## Test for non-linearity

In multiple linear regression, it is important for us to test the assumption of **linearity and additivity** of the relationship between dependent and independent variables.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

::: callout-note
The figure above reveals that most of the data points are **scattered around the 0 line**, hence we can safely conclude that the relationships between the dependent variable and independent variables are **linear**.
:::

## Test for Normality Assumption

```{r}
ols_plot_resid_hist(condo.mlr1)
```

::: callout-note
From the figure above, the **residual** of the multiple linear regression model resembles a **normal** distribution.
:::

Statistical Table:

```{r}
ols_test_normality(condo.mlr1)
```

::: callout-note
The summary table above reveals that the p-values of the four tests are way **smaller** than the alpha value of 0.05. Hence we will **reject** the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.
:::

## Testing for Spatial Autocorrelation

In order to perform spatial autocorrelation test, we need to convert *condo_resale.sf* from sf data frame into a **SpatialPointsDataFrame**.

1.  Export the residual of the hedonic pricing model and save it as a dataframe.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

2.  Join the newly created dataframe with condo_resale.sf object.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

3.  Next, we will convert *condo_resale.res.sf* from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

4.  Display the distribution with tmap

```{r}
tmap_mode("view")
tm_shape(mpsz)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

The figure above shows that there is signs of spatial autocorrelation.

We should conduct the Moran's I Test to proof that the observation is true.

### Moran's I Test

1.  Compute the distance-based weight matrix with spdep

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

2.  Convert the output neighbours list into spatial weights

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

3.  Perform the Moran's I test for residual spatial autocorrelation

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

::: callout-note
The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran I = 0.1424418 which is **greater than 0**, we can infer than the **residuals resemble cluster distribution**.
:::

# Building Hedonic Pricing Models with GWmodel

## Building Fixed Bandwidth GWR Model

### 1. Computing the fixed bandwidth

-   `bw.gwr()` is used to determine the optimal fixed bandwidth

-   adaptive = FALSE to indicate computing the **fixed** bandwidth

-   stopping rule can either be (i) CV cross-validation approach or (ii) AIC corrected approach. This rule is defined using the `approach` parameter.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

The recommended bandwidth is at 971.3405 metres. The model chooses the smallest number that give a reply.

### GWModel Method: Fixed Bandwidth

Calibrate the gwr model using the fixed bandwidth and gaussian kernel

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

The output is saved as "gwr". Display the output:

```{r}
gwr.fixed
```

The report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the global multiple linear regression model of 42967.1.

## Building an Adaptive Bandwidth GWR Model

-   Set adaptive = TRUE

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

The result shows that the 30 is the recommended data points to be used.

### Construct the adaptive bandwidth GWR model

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

```{r}
gwr.adaptive
```

The report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.

## Visualising GWR Output

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:

-   Condition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.

-   Local R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

-   Predicted: these are the estimated (or fitted) y values 3. computed by GWR.

-   Residuals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.

-   Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its "data" slot in an object called **SDF** of the output list.

## Converting SDF to sf dataframe

We have to convert SDF into sf dataframe to visualise the fields.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

```{r}
glimpse(condo_resale.sf.adaptive)
```

## Visualising local R2

```{r}
tmap_mode("view")
tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

## Visualising coefficient estimates

```{r}
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
```

### By URA Planning Region

```{r}
tm_shape(mpsz[mpsz$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

# Reference

# 

## 13.10 Reference

Gollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) "GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models". *Journal of Statistical Software*, 63(17):1-50, http://www.jstatsoft.org/v63/i17/

Lu B, Harris P, Charlton M, Brunsdon C (2014) "The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models". *Geo-spatial Information Science* 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453
